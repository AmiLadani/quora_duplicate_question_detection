{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reference: https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.data_utils import get_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('ttrain.csv')\n",
    "test_set=pd.read_csv('testt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for python 2, this encoding works with tokenizer, no need to specify unicode or 'utf-8'\n",
    "data['question1'] = data['question1'].apply(lambda x: (str(x)))\n",
    "data['question2'] = data['question2'].apply(lambda x: (str(x)))\n",
    "\n",
    "\n",
    "test_set['question1'] = test_set['question1'].apply(lambda x: (str(x)))\n",
    "test_set['question2'] = test_set['question2'].apply(lambda x: (str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target=data['is_duplicate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "question1 = list(data['question1'])\n",
    "question2 = list(data['question2'])\n",
    "\n",
    "test1=list(test_set['question1'])\n",
    "test2=list(test_set['question2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "343635\n",
      "60643\n"
     ]
    }
   ],
   "source": [
    "print(len(question1))\n",
    "print(len(test1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenize the corpus, then transform sentence into sequences of integer corresponding to tokenizer word index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=200000)\n",
    "tokenizer.fit_on_texts(question1+question2+test1+test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question1_word_sequences = tokenizer.texts_to_sequences(question1)\n",
    "question2_word_sequences = tokenizer.texts_to_sequences(question2)\n",
    "word_index = tokenizer.word_index #unique words in corpus (training and test sets)\n",
    "\n",
    "print(\"Words in index: %d\" % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test1_word_sequences = tokenizer.texts_to_sequences(test1)\n",
    "test2_word_sequences = tokenizer.texts_to_sequences(test2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_data = pad_sequences(question1_word_sequences, maxlen=25)\n",
    "q2_data = pad_sequences(question2_word_sequences, maxlen=25)\n",
    "test1_data=pad_sequences(test1_word_sequences, maxlen=25)\n",
    "test2_data=pad_sequences(test2_word_sequences, maxlen=25)\n",
    "\n",
    "labels = np.array(target, dtype=int)\n",
    "print('Shape of question1 data tensor:', q1_data.shape)\n",
    "print('Shape of question2 data tensor:', q2_data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  word embedding dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "f = open('glove.840B.300d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save as numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(open('q1_train.npy', 'wb'), q1_data)\n",
    "np.save(open('q2_train.npy', 'wb'), q2_data)\n",
    "np.save(open('test1.npy', 'wb'), test1_data)\n",
    "np.save(open('test2.npy', 'wb'), test2_data)\n",
    "\n",
    "np.save(open('label_train.npy', 'wb'), labels)\n",
    "np.save(open('word_embedding_matrix.npy', 'wb'), embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation,GlobalAveragePooling1D,Lambda,Bidirectional\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras import backend as B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q1_data = np.load(open('q1_train.npy', 'rb'))\n",
    "q2_data = np.load(open('q2_train.npy', 'rb'))\n",
    "\n",
    "labels = np.load(open('label_train.npy', 'rb'))\n",
    "embedding_matrix = np.load(open('word_embedding_matrix.npy', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X = np.stack((q1_data, q2_data), axis=1)\n",
    "target = labels\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, target, test_size=0.25, random_state=126, stratify=target)\n",
    "Q1_train = X_train[:,0]\n",
    "Q2_train = X_train[:,1]\n",
    "Q1_val = X_val[:,0]\n",
    "Q2_val = X_val[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vec_distance(vects):\n",
    "    x, y = vects\n",
    "    return B.sum(B.square(x - y), axis=1, keepdims=True)\n",
    "#don't use squar root of the sum, it doens't give a good range to feed to the dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vec_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "nb_words=137077+1\n",
    "max_sentence_len=25\n",
    "embedding_layer = Embedding(nb_words,300,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=max_sentence_len,trainable=False)\n",
    "#dont train this layer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm_layer =LSTM(128)\n",
    "\n",
    "sequence_1_input = Input(shape=(max_sentence_len,), dtype='int32')\n",
    "embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "x1 = lstm_layer(embedded_sequences_1)\n",
    "\n",
    "sequence_2_input = Input(shape=(max_sentence_len,), dtype='int32')\n",
    "embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "y1 = lstm_layer(embedded_sequences_2)\n",
    "\n",
    "distance=Lambda(vec_distance, output_shape=vec_output_shape)([x1, y1])\n",
    "dense1=Dense(16, activation='sigmoid')(distance)\n",
    "dense1 = Dropout(0.3)(dense1)\n",
    "\n",
    "bn2 = BatchNormalization()(dense1)\n",
    "prediction=Dense(1, activation='sigmoid')(bn2)\n",
    "\n",
    "model = Model(input=[sequence_1_input, sequence_2_input], output=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#according to Keras, RMSprop (adaptive LR) is good for recurrent neural net. \n",
    "# Adam is another method that computes adaptive learning rates for each parameter. \n",
    "#In addition to storing an exponentially decaying average of past squared gradients vtvt like Adadelta and RMSprop, Adam also keeps an exponentially decaying average of past gradients mtmt,\n",
    "\n",
    "###RMSprop as well divides the learning rate by an exponentially decaying average of squared gradients. \n",
    "##Adam is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, \n",
    "#and is well suited for problems that are large in terms of data and/or parameter\n",
    "model.compile(loss='binary_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "early_stopping =EarlyStopping(monitor='val_loss', patience=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#optional: try calculating class weights\n",
    "#source: stack exchange, J.Guillaumin\n",
    "\n",
    "import math\n",
    "\n",
    "# labels_dict : {ind_label: count_label}\n",
    "# mu : parameter to tune \n",
    "\n",
    "def create_class_weight(labels_dict,mu=2):\n",
    "    total = np.sum(labels_dict.values())\n",
    "    keys = labels_dict.keys()\n",
    "    class_weight = dict()\n",
    "\n",
    "    for key in keys:\n",
    "        score = math.log(mu*total/float(labels_dict[key]))\n",
    "        class_weight[key] = score if score > 1.0 else 1.0\n",
    "\n",
    "    return class_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####optional, assign weights to the labels due to imbalanced labels (0,1)\n",
    "unique, counts = np.unique(target, return_counts=True)\n",
    "labels_dict=dict(zip(unique, counts))\n",
    "\n",
    "target_weight=create_class_weight(labels_dict)\n",
    "target_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hist=model.fit([Q1_train, Q2_train], y_train, validation_data=([Q1_val, Q2_val], y_val), verbose=1, \n",
    "          nb_epoch=1, batch_size=256, shuffle=True,class_weight=None, callbacks=[early_stopping])\n",
    "#takes long time to initiate\n",
    "#using dense() layer and sigmoid activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# export model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"brnn_model_distance_128_d16_d05.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"brnn_model_distance_128_d16_d05.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "json_file = open('brnn_model_distance_128_d16_d05.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "model.load_weights(\"brnn_model_distance_128_d16_d05.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test1_data = np.load(open('test1.npy', 'rb'))\n",
    "test2_data = np.load(open('test2.npy', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred=model.predict([test1_data, test2_data],verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission=pd.read_csv('sample.csv')\n",
    "print pred.clip(1e-5, 0.99999)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission['is_duplicate']=pred.clip(1e-5, 0.99999)\n",
    "submission.to_csv('lstm_submission(13).csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
